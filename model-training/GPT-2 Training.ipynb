{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ce7731",
   "metadata": {},
   "source": [
    "# Training GPT-2 model on Bangla books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35974f96",
   "metadata": {},
   "source": [
    "Necessary imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5765a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c3e0",
   "metadata": {},
   "source": [
    "## Preparing dataset\n",
    "Initialize the project root, data, and model directories. `processed_data` directory contains selected books from renowned authors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04518bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and model directories\n",
    "project_root = Path('__file__').resolve().parents[1]\n",
    "data_dir = project_root / 'processed_data/'\n",
    "model_dir = project_root / 'models/bn_gpt2'\n",
    "os.makedirs(model_dir, exist_ok=True) # Create if does not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82779262",
   "metadata": {},
   "source": [
    "Get list of files in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fef3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [str(file) for file in Path(data_dir).glob('**/*.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a375843",
   "metadata": {},
   "source": [
    "Then we randomly shuffle the filenames list and select 100 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f03d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(filenames)\n",
    "filenames = filenames[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ad487",
   "metadata": {},
   "source": [
    "Split the files for training and testing. We are keeping 20% for testing and 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53afea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.2\n",
    "train_size = int(len(filenames) * train_test_split)\n",
    "training_filenames = filenames[:-train_size]\n",
    "test_filenames = filenames[-train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f61d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tokenizer, files, max_sequence_length):\n",
    "    # create inputs and labels\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                string_tokenized = tokenizer.encode(line)\n",
    "                # create a list of block size tokens\n",
    "                examples = []\n",
    "                BATCH_SIZE = 12\n",
    "                BUFFER_SIZE = 1000\n",
    "                for i in range(0, len(string_tokenized) - max_sequence_length + 1, max_sequence_length):\n",
    "                    ex = string_tokenized[i:i + max_sequence_length]\n",
    "                    inputs.append(ex[:-1])\n",
    "                    labels.append(ex[1:])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662c10e",
   "metadata": {},
   "source": [
    "Create a tokenizer from pretrained model and add special tokens for padding, beginning, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b72e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n",
    "tokenizer.add_special_tokens(\n",
    "    {'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a093e",
   "metadata": {},
   "source": [
    "Define max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea431a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6246d72",
   "metadata": {},
   "source": [
    "Now we create training dataset `X_train` and `y_train` from training files and test dataset `X_test` and `y_test` from test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2dcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:33<00:00,  2.40it/s]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:05<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset(tokenizer, training_filenames, MAX_SEQUENCE_LENGTH)\n",
    "test_dataset = create_dataset(tokenizer, test_filenames, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d4d52",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41676607",
   "metadata": {},
   "source": [
    "Load pretrained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f6fb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:39.365952: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('flax-community/gpt2-bengali', from_pt=True)\n",
    "# create model parameters\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(loss=[loss, *[None] * model.config.n_layer], optimizer=adam, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63260cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(str(model_dir),\n",
    "                                                monitor='loss',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 119/2563 [>.............................] - ETA: 25:59 - loss: 1.6319 - logits_loss: 1.6319 - logits_accuracy: 0.5983 - past_key_values_1_accuracy: 1.3240e-04 - past_key_values_2_accuracy: 1.1099e-04 - past_key_values_3_accuracy: 2.0381e-05 - past_key_values_4_accuracy: 2.7419e-04 - past_key_values_5_accuracy: 1.4384e-04 - past_key_values_6_accuracy: 1.7859e-04 - past_key_values_7_accuracy: 1.1231e-04 - past_key_values_8_accuracy: 1.0528e-04 - past_key_values_9_accuracy: 1.0293e-04 - past_key_values_10_accuracy: 3.2976e-04 - past_key_values_11_accuracy: 7.7271e-05 - past_key_values_12_accuracy: 1.1422e-04"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=3, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(model_dir / 'history'), 'wb') as file_pi:\n",
    "    pickle.dump(history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "output_model_file = os.path.join(model_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(model_dir, CONFIG_NAME)\n",
    "# save model and model configs\n",
    "model.save_pretrained(model_dir)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

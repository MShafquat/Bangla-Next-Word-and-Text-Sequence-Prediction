{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training GPT-2 model on Bangla books"
      ],
      "metadata": {},
      "id": "a2ce7731"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports for the project"
      ],
      "metadata": {},
      "id": "35974f96"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "from tqdm import tqdm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2022-04-04 10:28:17.728338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n/anaconda/envs/Project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1649068100676
        }
      },
      "id": "5765a509"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing dataset\n",
        "Initialize the project root, data, and model directories. `processed_data` directory contains selected books from renowned authors only."
      ],
      "metadata": {},
      "id": "0383c3e0"
    },
    {
      "cell_type": "code",
      "source": [
        "# get data and model directories\n",
        "project_root = Path('__file__').resolve().parents[1]\n",
        "data_dir = project_root / 'processed_data/'\n",
        "model_dir = project_root / 'models/bn_gpt2'\n",
        "os.makedirs(model_dir, exist_ok=True) # Create if does not exist"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1649068108545
        }
      },
      "id": "04518bdc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get list of files in the data directory"
      ],
      "metadata": {},
      "id": "82779262"
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [str(file) for file in Path(data_dir).glob('**/*.txt')]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1649068113493
        }
      },
      "id": "3fef3ebe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we randomly shuffle the filenames list and select 100 files."
      ],
      "metadata": {},
      "id": "3a375843"
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(filenames)\n",
        "filenames = filenames[:100]"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1649068553775
        }
      },
      "id": "b1f03d8a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the files for training, validation, and testing. We are using 70% for training, 15% for validation and 15% for testing."
      ],
      "metadata": {},
      "id": "5b1ad487"
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(filenames) * 0.70)\n",
        "train_filenames = filenames[:train_size]\n",
        "\n",
        "validation_size = train_size + int(len(filenames) * 0.15)\n",
        "validation_filenames = filenames[train_size:validation_size]\n",
        "\n",
        "test_filenames = filenames[validation_size:]"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1649072798221
        }
      },
      "id": "f53afea6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`create_dataset` creates a Tensorflow Dataset from a list of filenames with a length of `max_sequence_length - 1`. `inputs` list contains first `max_sequence_length - 1` number of words except the last word, and `labels` list contains last `max_sequence_length - 1` words except the first word. `skip_size` defines how many words to skip from a tokenized string when creating `inputs` and `labels` list."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "aa8b522c-3fbc-4050-913c-c0c43cfdeed9"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(tokenizer, filenames, max_sequence_length, skip_size):\n",
        "    BATCH_SIZE = 12     # batch size 12 is needed for transformer\n",
        "    BUFFER_SIZE = 1000  # for random shuffling\n",
        "    inputs = []\n",
        "    labels = []\n",
        "\n",
        "    for file in tqdm(filenames):\n",
        "        with open(file, 'r') as f:\n",
        "            for line in f:\n",
        "                string_tokenized = tokenizer.encode(line)\n",
        "                for i in range(0, len(string_tokenized) - max_sequence_length + 1, skip_size):\n",
        "                    ex = string_tokenized[i:i + max_sequence_length]\n",
        "                    inputs.append(ex[:-1])\n",
        "                    labels.append(ex[1:])\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1649068123147
        }
      },
      "id": "84f61d16"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tokenizer from pretrained model and add special tokens for padding, beginning, and end of sentence."
      ],
      "metadata": {},
      "id": "e662c10e"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n",
        "tokenizer.add_special_tokens(\n",
        "    {'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>'})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "3"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1649068136687
        }
      },
      "id": "d8b72e67"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define max sequence length and skip size for `create_dataset`."
      ],
      "metadata": {},
      "id": "bf9a093e"
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 200\n",
        "SKIP_SIZE = 50"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1649068138704
        }
      },
      "id": "7ea431a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create train, validation and test dataset from the files."
      ],
      "metadata": {},
      "id": "a6246d72"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(tokenizer, train_filenames, MAX_SEQUENCE_LENGTH, SKIP_SIZE)\n",
        "validation_dataset = create_dataset(tokenizer, validation_filenames, MAX_SEQUENCE_LENGTH, SKIP_SIZE)\n",
        "test_dataset = create_dataset(tokenizer, test_filenames, MAX_SEQUENCE_LENGTH, SKIP_SIZE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 14/14 [00:09<00:00,  1.46it/s]\n100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n100%|██████████| 3/3 [00:01<00:00,  2.77it/s]\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "gather": {
          "logged": 1649072877251
        }
      },
      "id": "0e2dcee0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Model"
      ],
      "metadata": {},
      "id": "766d4d52"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained Bangla GPT2 model from [flax-community/gpt2-bengali](https://huggingface.co/flax-community/gpt2-bengali). Compile the pretrained model with optimizer `adam` with a learning_rate of 0.001, loss `sparse categorical crossentropy` and accuracy metric `sparse categorical accuracy`."
      ],
      "metadata": {},
      "id": "41676607"
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel.from_pretrained('flax-community/gpt2-bengali', from_pt=True)\n",
        "# create model parameters\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(loss=[loss, *[None] * model.config.n_layer], optimizer=adam, metrics=[metric])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'lm_head.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias']\n- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1649072880884
        }
      },
      "id": "7f6fb7ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a checkpoint to save the best model that has the lowest loss."
      ],
      "metadata": {},
      "id": "36aca902"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(str(model_dir),\n",
        "                                                monitor='loss',\n",
        "                                                verbose=1,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='min')"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1649072881043
        }
      },
      "id": "34b0056e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train on `train_dataset` for some epochs."
      ],
      "metadata": {},
      "id": "1014abb8"
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=validation_dataset,\n",
        "                    epochs=10, batch_size=1024, callbacks=[checkpoint])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649072836534
        }
      },
      "id": "19aeda02"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training each epoch took around 30 minutes. Now we save the history for later use."
      ],
      "metadata": {},
      "id": "ea11d63d"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(str(model_dir / 'history'), 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1649097600269
        }
      },
      "id": "92eda952"
    },
    {
      "cell_type": "markdown",
      "source": [
        "And also save the model and tokenizer."
      ],
      "metadata": {},
      "id": "5b5c18eb"
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "output_model_file = os.path.join(model_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(model_dir, CONFIG_NAME)\n",
        "# save model and model configs\n",
        "model.save_pretrained(model_dir)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "# save tokenizer\n",
        "tokenizer.save_pretrained(model_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649097608520
        }
      },
      "id": "7931e775"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we evaluate the model on `test_dataset`"
      ],
      "metadata": {},
      "id": "12ae5227"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset, batch_size=1024)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649097641843
        }
      },
      "id": "3d7d774d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first value in the list is the loss, and the 3rd one is the accuracy."
      ],
      "metadata": {},
      "id": "4e004dbf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the training accuracy and loss"
      ],
      "metadata": {},
      "id": "3a3fb80e"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['logits_accuracy'])\n",
        "plt.plot(history.history['val_logits_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "fig.savefig(str(model_dir / 'logits_accuracy.png'), dpi=fig.dpi)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['logits_loss'])\n",
        "plt.plot(history.history['val_logits_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "fig.savefig(str(model_dir / 'logits_loss.png'), dpi=fig.dpi)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649097678543
        }
      },
      "id": "0fd8a9ff"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model with actual data"
      ],
      "metadata": {},
      "id": "4da9caf2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer and the model first if not already loaded"
      ],
      "metadata": {},
      "id": "a70e88b4"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(str(model_dir))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649072836641
        }
      },
      "id": "e3a9241b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `generate_text` takes as input text, model, and tokenizer and generates top 5 texts with sampling `top_k` of 50 and `top_p` of 0.95. More on these sampling can be found on this [blog post](https://huggingface.co/blog/how-to-generate)."
      ],
      "metadata": {},
      "id": "d666b5c9"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(text, model, tokenizer):\n",
        "    input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "    outputs = model.predict(input_ids).logits\n",
        "\n",
        "    print(\"Next most probable tokens:\\n\" + 100 * '-')\n",
        "    for i in range(outputs.shape[1]):\n",
        "        pred_id = np.argmax(outputs[:, i, :]).item()\n",
        "        print(tokenizer.decode(pred_id))\n",
        "    \n",
        "    beam_outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_return_sequences=5,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "    print(\"Beam Output:\\n\" + 100 * '-')\n",
        "    for i, beam_output in enumerate(beam_outputs):\n",
        "        print(\"{}: {}\".format(i, tokenizer.decode(\n",
        "            beam_output, skip_special_tokens=True)))"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1649097729448
        }
      },
      "id": "15d937c4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our model for different texts using this function"
      ],
      "metadata": {},
      "id": "3ab42530"
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter text: \")\n",
        "generate_text(text, model, tokenizer)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649097798173
        }
      },
      "id": "85103091"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting tensorflow model to tflite format"
      ],
      "metadata": {},
      "id": "00b60bd4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set converter parameters to convert the model with FP16 quantization"
      ],
      "metadata": {},
      "id": "0ecbabe1"
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649072836707
        }
      },
      "id": "fbe3c1f0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now convert the model and save as `.tflite` file"
      ],
      "metadata": {},
      "id": "e5b78b3e"
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model = converter.convert()\n",
        "# Save the model.\n",
        "with open(model_dir / 'bn_gpt2_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649072836726
        }
      },
      "id": "6f642750"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model can be used with android applications"
      ],
      "metadata": {},
      "id": "49225595"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "project",
      "language": "python",
      "display_name": "Project Setup"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "project"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}